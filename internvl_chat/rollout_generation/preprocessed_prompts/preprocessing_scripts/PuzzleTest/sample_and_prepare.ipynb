{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 JSON files:\n",
      "  - circle_size_number.json\n",
      "  - color_grid.json\n",
      "  - color_hexagon.json\n",
      "  - color_number_hexagon.json\n",
      "  - color_overlap_squares.json\n",
      "  - color_size_circle.json\n",
      "  - grid_number.json\n",
      "  - grid_number_color.json\n",
      "  - polygon_sides_color.json\n",
      "  - polygon_sides_number.json\n",
      "  - rectangle_height_color.json\n",
      "  - rectangle_height_number.json\n",
      "  - shape_morph.json\n",
      "  - shape_reflect.json\n",
      "  - shape_size_grid.json\n",
      "  - shape_size_hexagon.json\n",
      "  - size_cycle.json\n",
      "  - size_grid.json\n",
      "  - triangle.json\n",
      "  - venn.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Path to the AlgoPuzzleVQA data directory\n",
    "# data_dir = \"AlgoPuzzleVQA/data\"\n",
    "data_dir = \"PuzzleVQA/data\"\n",
    "\n",
    "# Find all JSON files in the data directory\n",
    "json_files = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.json'):\n",
    "        json_files.append(file)\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files:\")\n",
    "for file in json_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total files: 20\n",
      "Split point: 10\n",
      "\n",
      "Training files (10):\n",
      "  - venn.json\n",
      "  - color_size_circle.json\n",
      "  - shape_size_grid.json\n",
      "  - color_overlap_squares.json\n",
      "  - polygon_sides_number.json\n",
      "  - shape_reflect.json\n",
      "  - shape_size_hexagon.json\n",
      "  - triangle.json\n",
      "  - grid_number.json\n",
      "  - shape_morph.json\n",
      "\n",
      "Testing files (10):\n",
      "  - size_grid.json\n",
      "  - rectangle_height_color.json\n",
      "  - color_grid.json\n",
      "  - rectangle_height_number.json\n",
      "  - color_hexagon.json\n",
      "  - size_cycle.json\n",
      "  - grid_number_color.json\n",
      "  - polygon_sides_color.json\n",
      "  - circle_size_number.json\n",
      "  - color_number_hexagon.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate split point (half of total files)\n",
    "total_files = len(json_files)\n",
    "split_point = total_files // 2\n",
    "\n",
    "# Randomly split into training and testing sets\n",
    "random.shuffle(json_files)\n",
    "train_files = json_files[:split_point]\n",
    "test_files = json_files[split_point:]\n",
    "\n",
    "print(f\"\\nTotal files: {total_files}\")\n",
    "print(f\"Split point: {split_point}\")\n",
    "print(f\"\\nTraining files ({len(train_files)}):\")\n",
    "for file in train_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "print(f\"\\nTesting files ({len(test_files)}):\")\n",
    "for file in test_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json_files(file_list, output_file):\n",
    "    \"\"\"Flatten multiple JSON files into a single JSONL file\"\"\"\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for json_file in file_list:\n",
    "            file_path = os.path.join(data_dir, json_file)\n",
    "            print(f\"Processing {json_file}...\")\n",
    "            \n",
    "            with open(file_path, 'r') as infile:\n",
    "                for line in infile:\n",
    "                    line = line.strip()\n",
    "                    if line:  # Skip empty lines\n",
    "                        # Parse the JSON object and write it to the output file\n",
    "                        try:\n",
    "                            json_obj = json.loads(line)\n",
    "                            # Add source_file field to track origin\n",
    "                            json_obj[\"source_file\"] = json_file\n",
    "                            outfile.write(json.dumps(json_obj) + '\\n')\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Warning: Could not parse JSON in {json_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating training JSONL file...\n",
      "Processing venn.json...\n",
      "Processing color_size_circle.json...\n",
      "Processing shape_size_grid.json...\n",
      "Processing color_overlap_squares.json...\n",
      "Processing polygon_sides_number.json...\n",
      "Processing shape_reflect.json...\n",
      "Processing shape_size_hexagon.json...\n",
      "Processing triangle.json...\n",
      "Processing grid_number.json...\n",
      "Processing shape_morph.json...\n"
     ]
    }
   ],
   "source": [
    "# Create training JSONL file\n",
    "print(\"\\nCreating training JSONL file...\")\n",
    "flatten_json_files(train_files, \"PuzzleVQA_train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating testing JSONL file...\n",
      "Processing size_grid.json...\n",
      "Processing rectangle_height_color.json...\n",
      "Processing color_grid.json...\n",
      "Processing rectangle_height_number.json...\n",
      "Processing color_hexagon.json...\n",
      "Processing size_cycle.json...\n",
      "Processing grid_number_color.json...\n",
      "Processing polygon_sides_color.json...\n",
      "Processing circle_size_number.json...\n",
      "Processing color_number_hexagon.json...\n"
     ]
    }
   ],
   "source": [
    "# Create testing JSONL file\n",
    "print(\"\\nCreating testing JSONL file...\")\n",
    "flatten_json_files(test_files, \"PuzzleVQA_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training file: PuzzleVQA_train.jsonl (1000 records)\n",
      "Testing file: PuzzleVQA_test.jsonl (1000 records)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Count lines in output files\n",
    "def count_lines(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return sum(1 for line in f)\n",
    "\n",
    "# print(f\"\\nTraining file: AlgoPuzzleVQA_train.jsonl ({count_lines('AlgoPuzzleVQA_train.jsonl')} records)\")\n",
    "# print(f\"Testing file: AlgoPuzzleVQA_test.jsonl ({count_lines('AlgoPuzzleVQA_test.jsonl')} records)\")\n",
    "print(f\"\\nTraining file: PuzzleVQA_train.jsonl ({count_lines('PuzzleVQA_train.jsonl')} records)\")\n",
    "print(f\"Testing file: PuzzleVQA_test.jsonl ({count_lines('PuzzleVQA_test.jsonl')} records)\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from full_vqav2_en_20240402_int.jsonl...\n",
      "Loaded 5423 records\n",
      "Sample record: {'image': 'MMPR-v1.2-prompts/correctness_images/vqav2_en_20240402_int/COCO_train2014_000000000086.jpg', 'question': 'What is the number on the mailbox? Answer the question using a single word or phrase.', 'answer': '24'}\n"
     ]
    }
   ],
   "source": [
    "# Load the full JSONL file\n",
    "input_file = \"full_vqav2_en_20240402_int.jsonl\"\n",
    "data = []\n",
    "\n",
    "print(f\"Loading data from {input_file}...\")\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(data)} records\")\n",
    "print(\"Sample record:\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4451 unique images\n"
     ]
    }
   ],
   "source": [
    "# Group by unique images to ensure no duplicates\n",
    "unique_images = {}\n",
    "for record in data:\n",
    "    image_path = record[\"image\"]\n",
    "    if image_path not in unique_images:\n",
    "        unique_images[image_path] = record\n",
    "\n",
    "print(f\"Found {len(unique_images)} unique images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 4000 records for subset\n"
     ]
    }
   ],
   "source": [
    "# Sample 15K unique records\n",
    "sample_size = 4000\n",
    "unique_records = list(unique_images.values())\n",
    "\n",
    "if len(unique_records) < sample_size:\n",
    "    print(f\"Warning: Only {len(unique_records)} unique images available, less than requested {sample_size}\")\n",
    "    sampled_data = unique_records\n",
    "else:\n",
    "    # Use random.sample for sampling without replacement\n",
    "    random.seed(42)  # For reproducibility\n",
    "    sampled_data = random.sample(unique_records, sample_size)\n",
    "\n",
    "print(f\"Selected {len(sampled_data)} records for subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding UIDs and saving subset...\n",
      "Saved 4000 records to vqav2_int_only_4K_v1_subset.jsonl\n",
      "Sample record with UID: {'image': 'MMPR-v1.2-prompts/correctness_images/vqav2_en_20240402_int/COCO_train2014_000000287656.jpg', 'question': 'What number is the guy with the red number on his back? Answer the question using a single word or phrase.', 'answer': '24', 'uid': '6ac68031-7f0e-4949-b091-df92c0d1ff43'}\n"
     ]
    }
   ],
   "source": [
    "# Add UUID to each record and save subset\n",
    "subset_output_file = \"vqav2_int_only_4K_v1_subset.jsonl\"\n",
    "\n",
    "print(\"Adding UIDs and saving subset...\")\n",
    "with open(subset_output_file, 'w') as f:\n",
    "    for record in sampled_data:\n",
    "        # Add UID to the record\n",
    "        record_with_uid = record.copy()\n",
    "        record_with_uid[\"uid\"] = str(uuid.uuid4())\n",
    "        \n",
    "        # Write to file\n",
    "        f.write(json.dumps(record_with_uid) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(sampled_data)} records to {subset_output_file}\")\n",
    "print(\"Sample record with UID:\", json.loads(open(subset_output_file).readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_source_image_path:  /data/users/brandon/ob1-projects/MMPR-v1.2-prompts/correctness_images/vqav2_en_20240402_int/COCO_train2014_000000287656.jpg\n",
      "image_filename:  COCO_train2014_000000287656.jpg\n"
     ]
    }
   ],
   "source": [
    "source_image_dir = \"/data/users/brandon/ob1-projects/MMPR-v1.2-prompts\"\n",
    "loaded_image_path = \"MMPR-v1.2-prompts/correctness_images/vqav2_en_20240402_int/COCO_train2014_000000287656.jpg\"\n",
    "\n",
    "source_image_path = \"/\".join(loaded_image_path.split(\"/\")[1::])\n",
    "full_source_image_path = os.path.join(source_image_dir, source_image_path)\n",
    "\n",
    "print(\"full_source_image_path: \", full_source_image_path)\n",
    "# print(\"loaded_image_path: \", loaded_image_path)\n",
    "\n",
    "\n",
    "image_filename = os.path.basename(loaded_image_path)\n",
    "print(\"image_filename: \", image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images to subset_images directory...\n",
      "Copied 1000 images so far...\n",
      "Copied 2000 images so far...\n",
      "Copied 3000 images so far...\n",
      "Copied 4000 images so far...\n",
      "Successfully copied 4000 images\n",
      "Failed to copy 0 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Copy images to subset_images directory\n",
    "subset_output_file = \"vqav2_int_only_4K_v1_subset.jsonl\"\n",
    "subset_images_dir = \"subset_images\"\n",
    "os.makedirs(subset_images_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Copying images to {subset_images_dir} directory...\")\n",
    "\n",
    "# Read the subset file to get the image paths\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "with open(subset_output_file, 'r') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line.strip())\n",
    "        source_image_path = \"/\".join(record[\"image\"].split(\"/\")[1::])\n",
    "\n",
    "        source_image_dir = \"/data/users/brandon/ob1-projects/MMPR-v1.2-prompts\"\n",
    "\n",
    "        full_source_image_path = os.path.join(source_image_dir, source_image_path)\n",
    "\n",
    "        image_filename = os.path.basename(source_image_path)\n",
    "        \n",
    "        dest_image_path = os.path.join(subset_images_dir, image_filename)\n",
    "        \n",
    "        # Skip if destination already exists to avoid overwriting\n",
    "        if os.path.exists(dest_image_path):\n",
    "            print(f\"Skipping {image_filename} - already exists\")\n",
    "            copied_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(full_source_image_path):\n",
    "                shutil.copy2(full_source_image_path, dest_image_path)\n",
    "                copied_count += 1\n",
    "                if copied_count % 1000 == 0:  # Progress update every 1000 files\n",
    "                    print(f\"Copied {copied_count} images so far...\")\n",
    "            else:\n",
    "                print(f\"Warning: Source image not found: {full_source_image_path}\")\n",
    "                failed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {full_source_image_path}: {e}\")\n",
    "            failed_count += 1\n",
    "\n",
    "print(f\"Successfully copied {copied_count} images\")\n",
    "print(f\"Failed to copy {failed_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in subset_images: 4000\n"
     ]
    }
   ],
   "source": [
    "# open subset_images directory and count the number of files\n",
    "import os\n",
    "\n",
    "subset_images_dir = \"subset_images\"\n",
    "\n",
    "# Count the number of files in the directory\n",
    "num_files = len(os.listdir(subset_images_dir))\n",
    "print(f\"Number of files in {subset_images_dir}: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final JSONL with absolute image paths: vqav2_run1_int_only_4K_v1_subset.jsonl\n",
      "Created final output file: vqav2_run1_int_only_4K_v1_subset.jsonl\n",
      "Sample final record:\n",
      "{\n",
      "  \"image\": \"MMPR-v1.2-prompts/correctness_images/vqav2_en_20240402_int/COCO_train2014_000000287656.jpg\",\n",
      "  \"question\": \"What number is the guy with the red number on his back? Answer the question using a single word or phrase.\",\n",
      "  \"answer\": \"24\",\n",
      "  \"uid\": \"6ac68031-7f0e-4949-b091-df92c0d1ff43\",\n",
      "  \"image_path\": \"/data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/VQAv2/subset_images/COCO_train2014_000000287656.jpg\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Copy images to subset_images directory\n",
    "subset_output_file = \"vqav2_int_only_4K_v1_subset.jsonl\"\n",
    "# Create final JSONL with absolute image paths\n",
    "final_output_file = \"vqav2_run1_int_only_4K_v1_subset.jsonl\"\n",
    "base_dir = \"/data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/VQAv2\"\n",
    "absolute_subset_images_dir = os.path.join(base_dir, \"subset_images\")\n",
    "\n",
    "print(f\"Creating final JSONL with absolute image paths: {final_output_file}\")\n",
    "\n",
    "with open(subset_output_file, 'r') as input_f, open(final_output_file, 'w') as output_f:\n",
    "    for line in input_f:\n",
    "        record = json.loads(line.strip())\n",
    "        \n",
    "        # Extract original filename\n",
    "        image_filename = os.path.basename(record[\"image\"])\n",
    "        \n",
    "        # Create absolute path to the copied image\n",
    "        absolute_image_path = os.path.join(absolute_subset_images_dir, image_filename)\n",
    "        \n",
    "        # Add the image_path field\n",
    "        record[\"image_path\"] = absolute_image_path\n",
    "        \n",
    "        # Write the updated record\n",
    "        output_f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(f\"Created final output file: {final_output_file}\")\n",
    "\n",
    "# Show sample of final output\n",
    "with open(final_output_file, 'r') as f:\n",
    "    sample_record = json.loads(f.readline())\n",
    "    print(\"Sample final record:\")\n",
    "    print(json.dumps(sample_record, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
