{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"file-5a7b2c7922194f5083f7bae7e730f14b\",\n",
      "  \"bytes\": 1625,\n",
      "  \"created_at\": 1749630487,\n",
      "  \"filename\": \"example_batch_input.jsonl\",\n",
      "  \"object\": \"file\",\n",
      "  \"purpose\": \"batch\",\n",
      "  \"status\": \"processed\",\n",
      "  \"expires_at\": 1750840087,\n",
      "  \"status_details\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "    \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\"),  \n",
    "    api_version=\"2025-03-01-preview\",\n",
    "    # azure_endpoint = \"https://aisg-sj.openai.azure.com/\" # o4-mini\n",
    "    azure_endpoint = \"https://decla-mbncunfi-australiaeast.cognitiveservices.azure.com/\" # o3-mini\n",
    "    )\n",
    "\n",
    "# Upload a file with a purpose of \"batch\"\n",
    "file = client.files.create(\n",
    "  file=open(\"example_batch_input.jsonl\", \"rb\"), \n",
    "  purpose=\"batch\",\n",
    "  extra_body={\"expires_after\":{\"seconds\": 1209600, \"anchor\": \"created_at\"}} # Optional you can set to a number between 1209600-2592000. This is equivalent to 14-30 days\n",
    ")\n",
    "\n",
    "\n",
    "print(file.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File expiration: 2025-06-25 16:28:07\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"File expiration: {datetime.datetime.fromtimestamp(file.expires_at) if file.expires_at is not None else 'Not set'}\")\n",
    "\n",
    "file_id = file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4\",\n",
      "  \"completion_window\": \"24h\",\n",
      "  \"created_at\": 1749630496,\n",
      "  \"endpoint\": \"/chat/completions\",\n",
      "  \"input_file_id\": \"file-5a7b2c7922194f5083f7bae7e730f14b\",\n",
      "  \"object\": \"batch\",\n",
      "  \"status\": \"validating\",\n",
      "  \"cancelled_at\": null,\n",
      "  \"cancelling_at\": null,\n",
      "  \"completed_at\": null,\n",
      "  \"error_file_id\": \"\",\n",
      "  \"errors\": null,\n",
      "  \"expired_at\": null,\n",
      "  \"expires_at\": 1749716893,\n",
      "  \"failed_at\": null,\n",
      "  \"finalizing_at\": null,\n",
      "  \"in_progress_at\": null,\n",
      "  \"metadata\": null,\n",
      "  \"output_file_id\": \"\",\n",
      "  \"request_counts\": {\n",
      "    \"completed\": 0,\n",
      "    \"failed\": 0,\n",
      "    \"total\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Submit a batch job with the file\n",
    "batch_response = client.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    extra_body={\"output_expires_after\":{\"seconds\": 1209600, \"anchor\": \"created_at\"}} # Optional you can set to a number between 1209600-2592000. This is equivalent to 14-30 days\n",
    ")\n",
    "\n",
    "\n",
    "# Save batch ID for later use\n",
    "batch_id = batch_response.id\n",
    "\n",
    "print(batch_response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:29:16.871933 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: validating\n",
      "2025-06-11 16:30:17.298923 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: validating\n",
      "2025-06-11 16:31:17.726099 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: in_progress\n",
      "2025-06-11 16:32:18.166221 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: in_progress\n",
      "2025-06-11 16:33:18.590748 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: finalizing\n",
      "2025-06-11 16:34:19.021173 Batch Id: batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4,  Status: completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime \n",
    "\n",
    "status = \"validating\"\n",
    "while status not in (\"completed\", \"failed\", \"canceled\"):\n",
    "    time.sleep(60)\n",
    "    batch_response = client.batches.retrieve(batch_id)\n",
    "    status = batch_response.status\n",
    "    print(f\"{datetime.datetime.now()} Batch Id: {batch_id},  Status: {status}\")\n",
    "\n",
    "if batch_response.status == \"failed\":\n",
    "    for error in batch_response.errors.data:  \n",
    "        print(f\"Error code {error.code} Message {error.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"custom_id\": \"request-1\",\n",
      "  \"response\": {\n",
      "    \"request_id\": \"b2488486-f5b0-48c2-a231-89414b3574c6\",\n",
      "    \"status_code\": 400\n",
      "  },\n",
      "  \"error\": {\n",
      "    \"code\": null,\n",
      "    \"message\": {\n",
      "      \"error\": {\n",
      "        \"message\": \"Invalid content type. image_url is only supported by certain models.\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": \"messages.[1].content.[1].type\",\n",
      "        \"code\": null\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_file_id = batch_response.output_file_id\n",
    "\n",
    "if not output_file_id:\n",
    "    output_file_id = batch_response.error_file_id\n",
    "\n",
    "if output_file_id:\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')  \n",
    "\n",
    "    for raw_response in raw_responses:  \n",
    "        json_response = json.loads(raw_response)  \n",
    "        formatted_json = json.dumps(json_response, indent=2)  \n",
    "        print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch(id='batch_d4b38a98-4b8e-4ad0-b667-514f90a7dff4', completion_window='24h', created_at=1749630496, endpoint='/chat/completions', input_file_id='file-5a7b2c7922194f5083f7bae7e730f14b', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1749630852, error_file_id='file-f91d073c-a5cf-471f-8385-8ab04b14dc38', errors=None, expired_at=None, expires_at=1749716893, failed_at=None, finalizing_at=1749630768, in_progress_at=1749630655, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=1, total=1))]\n"
     ]
    }
   ],
   "source": [
    "all_jobs = []\n",
    "# Automatically fetches more pages as needed.\n",
    "for job in client.batches.list(\n",
    "    limit=20,\n",
    "):\n",
    "    # Do something with job here\n",
    "    all_jobs.append(job)\n",
    "print(all_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import BadRequestError\n",
    "\n",
    "max_retries = 10\n",
    "retries = 0\n",
    "initial_delay = 5\n",
    "delay = initial_delay\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        batch_response = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "        )\n",
    "        \n",
    "        # Save batch ID for later use\n",
    "        batch_id = batch_response.id\n",
    "        \n",
    "        print(f\"✅ Batch created successfully after {retries} retries\")\n",
    "        print(batch_response.model_dump_json(indent=2))\n",
    "        break  \n",
    "        \n",
    "    except BadRequestError as e:\n",
    "        error_message = str(e)\n",
    "        \n",
    "        # Check if it's a token limit error\n",
    "        if 'token_limit_exceeded' in error_message:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                print(f\"❌ Maximum retries ({max_retries}) reached. Giving up.\")\n",
    "                raise\n",
    "            \n",
    "            print(f\"⏳ Token limit exceeded. Waiting {delay} seconds before retry {retries}/{max_retries}...\")\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            # Exponential backoff - increase delay for next attempt\n",
    "            delay *= 2\n",
    "        else:\n",
    "            # If it's a different error, raise it immediately\n",
    "            print(f\"❌ Encountered non-token limit error: {error_message}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
