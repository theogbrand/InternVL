{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RAVEN Monte Carlo Rollout Process Inspection\n",
    "\n",
    "This notebook manually walks through the full Monte Carlo rollout process to help understand how the rollout generation works.\n",
    "\n",
    "## Process Overview:\n",
    "1. Load a sample from RAVEN dataset\n",
    "2. Generate initial rollout responses using the prompt template\n",
    "3. Parse responses into perception and reasoning steps\n",
    "4. Run Monte Carlo rollouts for each step with appropriate prefixes\n",
    "5. Score each MC rollout using parse_answer and check_answer functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Using deployment: gpt-4.1\n",
      "Using endpoint: https://declaregpt4.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# Import required modules and functions from rollout.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import threading\n",
    "from mimetypes import guess_type\n",
    "from pprint import pprint\n",
    "\n",
    "# Azure OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    "    wait_random\n",
    ")\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Add the parent directory to path to import rollout module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import specific functions from rollout.py (not the API functions)\n",
    "from rollout import (\n",
    "    RAVENDataset,\n",
    "    parse_response_to_perception_and_reasoning_steps_and_correct_answer\n",
    ")\n",
    "\n",
    "# Add the tools directory to import accuracy functions\n",
    "sys.path.append('/data/users/brandon/ob1-projects/InternVL/internvl_chat/tools')\n",
    "from reasoning_data_pipeline.utils.accuracy_reward import check_answer, parse_answer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('rollout_inspection')\n",
    "\n",
    "# Verify Azure API key is set\n",
    "if not os.getenv(\"AZURE_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_API_KEY environment variable not set!\")\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "endpoint = \"https://declaregpt4.openai.azure.com/\"\n",
    "deployment = \"gpt-4.1\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "# Create standalone Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_INSPECT_API_KEY\"),\n",
    "    timeout=60.0\n",
    ")\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Using deployment: {deployment}\")\n",
    "print(f\"Using endpoint: {endpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions copied from rollout.py\n",
    "\n",
    "def local_image_to_data_url(image_path):\n",
    "    \"\"\"Convert a local image into data URL\"\"\"\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=300) + wait_random(0, 30),\n",
    "    retry=retry_if_exception_type((Exception,)),\n",
    "    reraise=True\n",
    ")\n",
    "def make_azure_request(messages, max_tokens, temperature, estimated_tokens=1000):\n",
    "    \"\"\"Make Azure OpenAI request with retry logic\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            max_completion_tokens=max_tokens,\n",
    "            model=deployment,\n",
    "            temperature=temperature,\n",
    "            timeout=120.0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Check for content filter violation - DO NOT RETRY\n",
    "        if ('BadRequestError' in error_type and \n",
    "            'Error code: 400' in error_msg and \n",
    "            ('ResponsibleAIPolicyViolation' in error_msg or 'content_filter' in error_msg)):\n",
    "            logger.warning(f\"Content filter violation detected: {error_msg}\")\n",
    "            return \"Error code 400: content filter violation returned\"\n",
    "        \n",
    "        logger.debug(f\"API request failed: {error_type}: {error_msg}\")\n",
    "        raise\n",
    "\n",
    "def build_responses_azure_simple(inputs, num_return_sequences=1, prefixes=None, max_new_tokens=4096, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Simplified version of build_responses_azure for inspection purposes\n",
    "    \"\"\"\n",
    "    total_requests = len(inputs) * num_return_sequences\n",
    "    logger.info(f\"Generating {total_requests} responses\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for seq_idx in range(num_return_sequences):\n",
    "        for input_idx, (prompt, image_path) in enumerate(inputs):\n",
    "            try:\n",
    "                # Convert image path to data URL\n",
    "                data_url = local_image_to_data_url(image_path)\n",
    "                \n",
    "                # Prepare messages\n",
    "                content = [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "                ]\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": content}\n",
    "                ]\n",
    "                \n",
    "                # Add prefix if provided - handle both single prefix and list of prefixes\n",
    "                current_prefix = None\n",
    "                if prefixes:\n",
    "                    if isinstance(prefixes, list):\n",
    "                        # Multiple prefixes - use index based on current request\n",
    "                        request_idx = seq_idx * len(inputs) + input_idx\n",
    "                        if request_idx < len(prefixes) and prefixes[request_idx]:\n",
    "                            current_prefix = prefixes[request_idx]\n",
    "                    else:\n",
    "                        # Single prefix for all requests\n",
    "                        current_prefix = prefixes\n",
    "                \n",
    "                if current_prefix:\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": current_prefix})\n",
    "                \n",
    "                # Make the request\n",
    "                response_text = make_azure_request(messages, max_new_tokens, temperature)\n",
    "                responses.append(response_text)\n",
    "                \n",
    "                logger.info(f\"Generated response {len(responses)}/{total_requests}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate response {len(responses)+1}: {e}\")\n",
    "                responses.append(\"\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Load a Sample from RAVEN Dataset\n",
    "\n",
    "Let's load a single sample from the RAVEN dataset to inspect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2000 lines to 1 samples in range [1, 1]\n",
      "Loaded 1 samples from dataset\n",
      "\n",
      "Sample structure:\n",
      "- ID: 5571\n",
      "- Subset/Split: in_center_single_out_center_single\n",
      "- Correct Answer: G\n",
      "- Image Path: /data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/RAVEN/processed_raven_images/in_center_single_out_center_single/5571.jpg\n",
      "\n",
      "Rollout User Prompt Preview:\n",
      "You are an abstract reasoning puzzle expert. The puzzle you will receive is presented in a standard Raven's Progressive Matrices format: a 3×3 matrix of related images, with the bottom-right cell (the ninth tile) missing. There are eight possible answer choices provided separately, and your task is to decide which of those eight images correctly completes the 3×3 matrix pattern.\n",
      "\n",
      "I will provide you with an image containing:\n",
      "- Problem Matrix: An accompanying image that shows the eight tiles and h...\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset path - adjust this to your actual dataset\n",
    "dataset_path = '/data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/RAVEN/raven_processed_jsonl/last_four_jsonl/in_center_single_out_center_single_test.jsonl'\n",
    "\n",
    "# Load the dataset (just first few samples for inspection)\n",
    "dataset = RAVENDataset(\n",
    "    data=dataset_path,\n",
    "    sample_start_idx=1,\n",
    "    sample_end_idx=1  # Just load 3 samples for inspection\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples from dataset\")\n",
    "\n",
    "# Get the first sample\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"- ID: {sample['id']}\")\n",
    "print(f\"- Subset/Split: {sample['subset_split']}\")\n",
    "print(f\"- Correct Answer: {sample['correct_answer']}\")\n",
    "print(f\"- Image Path: {sample['image_path']}\")\n",
    "print(f\"\\nRollout User Prompt Preview:\")\n",
    "print(sample['rollout_user_prompt'][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Generate Initial Rollout Responses\n",
    "\n",
    "Now let's generate initial rollout responses using the Azure OpenAI API. We'll generate 2 responses to see variation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 2 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 initial rollout responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/2\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 2 responses\n",
      "\n",
      "================================================================================\n",
      "ROLLOUT RESPONSE 1:\n",
      "================================================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "The matrix is 3x3. The bottom-right cell is missing. There are eight answer choices (A-H) shown below the main matrix.\n",
      "</step_1>\n",
      "<step_2>\n",
      "Row 1: Each cell shows a circle containing a black shape:\n",
      "- 1st column: Black hexagon.\n",
      "- 2nd column: Black downward-pointing triangle.\n",
      "- 3rd column: Black pentagon.\n",
      "</step_2>\n",
      "<step_3>\n",
      "Row 2: Each cell shows an upright triangle containing a shape:\n",
      "- 1st column: Hexagon (outline, not filled).\n",
      "- 2nd column: Hexagon (outline, not filled), the triangle rotates or shifts.\n",
      "- 3rd column: Black triangle (filled), triangle rotates.\n",
      "</step_3>\n",
      "<step_4>\n",
      "Row 3: Each cell shows a square turned as a diamond (rotated square) with a shape inside:\n",
      "- 1st column: Outline triangle inside.\n",
      "- 2nd column: Black hexagon inside.\n",
      "- 3rd column: Cell is missing.\n",
      "</step_4>\n",
      "<step_5>\n",
      "Answer choices A-H: All are diamonds (rotated squares) with a shape inside, except D which is a circle with a hexagon inside.\n",
      "- A: Diamond with outlined hexagon inside.\n",
      "- B: Diamon...\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs for rollout generation\n",
    "num_return_sequences = 2  # Generate 2 different rollouts for the same input\n",
    "\n",
    "# Create input list (prompt, image_path)\n",
    "inputs = [(sample['rollout_user_prompt'], sample['image_path'])]\n",
    "\n",
    "print(f\"Generating {num_return_sequences} initial rollout responses...\")\n",
    "\n",
    "# Generate responses using the Azure API\n",
    "response_list = build_responses_azure_simple(\n",
    "    inputs=inputs,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    prefixes=None,  # No prefix for initial generation\n",
    "    max_new_tokens=8192,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(response_list)} responses\")\n",
    "\n",
    "# Display first response\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROLLOUT RESPONSE 1:\")\n",
    "print(\"=\"*80)\n",
    "print(response_list[0][:1000] + \"...\" if len(response_list[0]) > 1000 else response_list[0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Parse Response into Steps\n",
    "\n",
    "Let's parse the response to extract perception steps, reasoning steps, and the final answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed response!\n",
      "\n",
      "Number of perception steps: 4\n",
      "Number of reasoning steps: 3\n",
      "LLM Answer: $\\boxed{F}$\n",
      "Ground Truth Answer: G\n",
      "\n",
      "============================================================\n",
      "FIRST PERCEPTION STEP:\n",
      "============================================================\n",
      "I observe the first row of the matrix. Each cell depicts a large circle with a smaller shape inside. The shapes from left to right: a hexagon, a triangle, and a pentagon.\n",
      "\n",
      "============================================================\n",
      "FIRST REASONING STEP:\n",
      "============================================================\n",
      "Identify the logic in each row: The outer shape progresses as: circle (row 1), triangle (row 2), diamond (row 3). The inner shape shifts as well. Track the relationships of the inner shapes across rows and columns.\n"
     ]
    }
   ],
   "source": [
    "# Parse the first response\n",
    "# response_to_parse = response_list[0]\n",
    "response_to_parse = \"\"\"[Perception]\\n<step_1>\\nI observe the first row of the matrix. Each cell depicts a large circle with a smaller shape inside. The shapes from left to right: a hexagon, a triangle, and a pentagon.\\n</step_1>\\n<step_2>\\nI observe the second row. Each cell now shows a large triangle, containing a smaller shape inside. From left to right: a pentagon, a hexagon, and a triangle.\\n</step_2>\\n<step_3>\\nI observe the third row. Each cell presents a large diamond (square rotated 45 degrees with respect to the lower edge), with a smaller shape inside. The first contains a triangle, the middle one a hexagon, and the last one is missing.\\n</step_3>\\n<step_4>\\nI look at the answer choices A-H. Each shows a large diamond with a shape inside except D, which has a large circle. Shapes inside are various (hexagons, pentagons, squares) and some are filled, others are not.\\n</step_4>\\n\\n[Reasoning]\\n<step_1>\\nIdentify the logic in each row: The outer shape progresses as: circle (row 1), triangle (row 2), diamond (row 3). The inner shape shifts as well. Track the relationships of the inner shapes across rows and columns.\\n</step_1>\\n<step_2>\\nLook down each column:\\n- 1st column: Hexagon inside circle; pentagon inside triangle; triangle inside diamond.\\n- 2nd column: Triangle inside circle; hexagon inside triangle; hexagon inside diamond.\\n- 3rd column: Pentagon inside circle; triangle inside triangle; ? inside diamond.\\n</step_2>\\n<step_3>\\nNotice the pattern: In each column, the inner shape of the top row moves to the middle row, the middle row inner shape moves to the bottom row, and the bottom row inner shape moves to the top row (cyclical shift).\\n- 1st column: Hexagon (circle) → pentagon (triangle) → triangle (diamond)\\n- 2nd column: Triangle (circle) → hexagon (triangle) → hexagon (diamond)\\n- 3rd column: Pentagon (circle) → triangle (triangle) → ? (diamond)\\nSo, following the shift: Pentagon (circle) → triangle (triangle) → ? (diamond). The answer must have a pentagon inside the diamond.\\n</step_4>\\n<step_4>\\nExamine the answer choices for a diamond containing a pentagon. Option F is a diamond with a pentagon inside.\\n</step_4>\\n\\n<correct_answer>\\n$\\\\boxed{F}$\\n</correct_answer>\\n\"\"\"\n",
    "\n",
    "try:\n",
    "    parsed = parse_response_to_perception_and_reasoning_steps_and_correct_answer(\n",
    "        response_to_parse,\n",
    "        max_perception_steps=12,\n",
    "        max_reasoning_steps=12\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully parsed response!\")\n",
    "    print(f\"\\nNumber of perception steps: {len(parsed['perception_steps'])}\")\n",
    "    print(f\"Number of reasoning steps: {len(parsed['reasoning_steps'])}\")\n",
    "    print(f\"LLM Answer: {parsed['llm_answer']}\")\n",
    "    print(f\"Ground Truth Answer: {sample['correct_answer']}\")\n",
    "    \n",
    "    # Display first perception step\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST PERCEPTION STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['perception_steps'][0])\n",
    "   \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAST PERCEPTION STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['perception_steps'][-1])\n",
    "    \n",
    "    # Display first reasoning step\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST REASONING STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['reasoning_steps'][0])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAST REASONING STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['reasoning_steps'][-1])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse response: {e}\")\n",
    "    print(\"\\nResponse format may not match expected structure.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Run Monte Carlo Rollouts for Each Step\n",
    "\n",
    "Now we'll demonstrate the Monte Carlo process. For each step, we:\n",
    "1. Create a prefix containing all steps up to that point\n",
    "2. Generate multiple continuations from that prefix\n",
    "3. Parse answers and check correctness\n",
    "4. Calculate the MC score for that step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Monte Carlo for step 0 (Perception step 1)\n",
      "Will generate 4 MC continuations\n",
      "\n",
      "============================================================\n",
      "PREFIX FOR MC GENERATION:\n",
      "============================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "I observe the first row of the matrix. Each cell depicts a large circle with a smaller shape inside. The shapes from left to right: a hexagon, a triangle, and a pentagon.\n",
      "</step_1>\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's run MC for the first perception step (step index 0)\n",
    "step_idx = 0\n",
    "num_mc_sequences = 4  # Number of MC samples per step\n",
    "\n",
    "# Combine all steps for easier access\n",
    "all_steps = parsed['perception_steps'] + parsed['reasoning_steps']\n",
    "perception_count = len(parsed['perception_steps'])\n",
    "\n",
    "print(f\"Running Monte Carlo for step {step_idx} (Perception step {step_idx + 1})\")\n",
    "print(f\"Will generate {num_mc_sequences} MC continuations\")\n",
    "\n",
    "# Build the prefix for this step (all steps up to and including current step)\n",
    "prefix_steps = all_steps[:step_idx + 1]\n",
    "\n",
    "# Format the prefix according to the expected structure\n",
    "formatted_prefix = \"[Perception]\\n\"\n",
    "for i, step in enumerate(prefix_steps):\n",
    "    formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREFIX FOR MC GENERATION:\")\n",
    "print(\"=\"*60)\n",
    "print(formatted_prefix)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 4 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 4 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 MC responses\n",
      "\n",
      "================================================================================\n",
      "MC CONTINUATION 1 (after prefix):\n",
      "================================================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "The first row contains circles with different small shapes inside them:\n",
      "- First cell: Large circle with a small black hexagon inside.\n",
      "- Second cell: Large circle with a small downward triangle inside.\n",
      "- Third cell: Large circle with a small black pentagon inside.\n",
      "</step_1>\n",
      "<step_2>\n",
      "The second row contains large triangles with small shapes inside:\n",
      "- First cell: Large triangle with a small outlined pentagon inside.\n",
      "- Second cell: Large triangle with a small hexagon inside.\n",
      "- Third cell: Large triangle with a small triangle inside.\n",
      "</step_2>\n",
      "<step_3>\n",
      "The third row contains large diamonds (squares rotated 45 degrees) with small shapes inside:\n",
      "- First cell: Large diamond with a small outlined triangle inside.\n",
      "- Second cell: Large diamond with a small black hexagon inside.\n",
      "- Third cell: Large diamond, but the inside shape is missing (question mark).\n",
      "</step_3>\n",
      "<step_4>\n",
      "The answer set below contains 8 options (A-H), all large diamonds with a smaller shape inside (except D which is a circle with hexagon), with variations of the inside shape (hexagon, pentagon, triangle, or square; black-filled, outlined, grey, etc).\n",
      "</step_4>\n",
      "\n",
      "[Reasoning]\n",
      "<step_1>\n",
      "Each row changes the large outer shape while maintaining an inner shape with some transformation.\n",
      "First row (circles): inner shapes are black and solid (hexagon, triangle, pentagon).\n",
      "Second row (triangles): inner shapes mirror the previous row (from same columns) but become outlined or regular (first and second columns), or remain the same. The outer shape becomes a triangle.\n",
      "Third row (diamonds): outer shape is diamond, inner shape follows a similar transformation.\n",
      "</step_1>\n",
      "<step_2>\n",
      "Let's observe columnwise:\n",
      "- First column: Circle+black hexagon → Triangle+outlined pentagon → Diamond+outlined triangle.\n",
      "- Second column: Circle+black triangle → Triangle+hexagon → Diamond+black hexagon.\n",
      "- Third column: Circle+black pentagon → Triangle+triangle → Diamond+?\n",
      "This suggests a progression from hexagon→pentagon→triangle for the inner shapes as we go down each column. Also, the style alternates: black→outlined→outlined, or black→normal→black.\n",
      "</step_3>\n",
      "<step_3>\n",
      "Column 1: The interior shape goes from hexagon to pentagon to triangle (all changing downwards).\n",
      "Column 2: Triangle to hexagon to hexagon (change, then remain).\n",
      "Column 3: Pentagon to triangle to ?\n",
      "For the third column, first is pentagon, then triangle, so the next should be the shape that follows the sequence, which is a square—since the number of sides changes: hexagon (6 sides) → pentagon (5 sides) → triangle (3 sides) in column 1; triangle (3) → hexagon (6) → hexagon (6) in column 2; pentagon (5) → triangle (3) → ? in column 3.\n",
      "But based on this, the diamond row is always outlined triangle in col 1, black hexagon in col 2, so what fits best in col 3 for pattern?\n",
      "</step_4>\n",
      "<step_4>\n",
      "Another look: for each row, the inner shape from left to right: \n",
      "row 1: hexagon, triangle, pentagon\n",
      "row 2: pentagon, hexagon, triangle\n",
      "row 3: triangle, hexagon, ?\n",
      "That is: the shapes rotate left with each row.\n",
      "So, row 2 is row 1 rotated left.\n",
      "Row 3 is row 2 rotated left.\n",
      "So the answer for bottom right is pentagon, as that's the shape that was on the left cell in row 2, which was itself the left cell in row 1 last time.\n",
      "</step_4>\n",
      "<step_5>\n",
      "So, we need diamond with a small pentagon inside. Reviewing the answer options:\n",
      "A: diamond with hexagon \n",
      "B: diamond with outlined hexagon\n",
      "C: diamond with gray hexagon\n",
      "D: circle with hexagon\n",
      "E: diamond with small hexagon\n",
      "F: diamond with gray hexagon\n",
      "G: diamond with black hexagon\n",
      "H: diamond with square\n",
      "None of the options shows a pentagon. Let's review again. The pattern may not be pentagon, but hexagon.\n",
      "On close inspection, the actual sequence is: \n",
      "row 1: hexagon, triangle, hexagon\n",
      "row 2: pentagon, hexagon, triangle\n",
      "row 3: triangle, hexagon, ?\n",
      "\n",
      "Ah, the first row is: hexagon (6), triangle (3), pentagon (5)\n",
      "second row: pentagon (5), hexagon (6), triangle (3)\n",
      "third row: triangle (3), pentagon (5), hexagon (6)\n",
      "So it is actually cycling through the numbers of sides: 6→3→5 (row 1), 5→6→3 (row 2), 3→5→6 (row 3)\n",
      "Therefore, the missing shape is a hexagon, black-filled (to match the style in the same row/column).\n",
      "Matching with the answer choices, G is the diamond with black hexagon, which matches the other filled answers in its column and row.\n",
      "</step_5>\n",
      "\n",
      "<correct_answer>\n",
      "$\\boxed{G}$\n",
      "</correct_answer>\n"
     ]
    }
   ],
   "source": [
    "# Generate MC continuations from this prefix\n",
    "mc_inputs = [(sample['rollout_user_prompt'], sample['image_path'])] * num_mc_sequences\n",
    "mc_prefixes = [formatted_prefix] * num_mc_sequences\n",
    "\n",
    "print(f\"\\nGenerating {num_mc_sequences} MC continuations...\")\n",
    "\n",
    "mc_responses = build_responses_azure_simple(\n",
    "    inputs=mc_inputs,\n",
    "    num_return_sequences=1,  # 1 response per input (we duplicate inputs instead)\n",
    "    prefixes=mc_prefixes,\n",
    "    max_new_tokens=8192,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(mc_responses)} MC responses\")\n",
    "\n",
    "# Display first MC continuation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MC CONTINUATION 1 (after prefix):\")\n",
    "print(\"=\"*80)\n",
    "print(mc_responses[0][:500] + \"...\" if len(mc_responses[0]) < 500 else mc_responses[0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Parse MC Answers and Calculate Scores\n",
    "\n",
    "Now we'll extract the final answers from each MC continuation and check their correctness:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Answer: G\n",
      "\n",
      "Processing MC responses:\n",
      "------------------------------------------------------------\n",
      "MC 0: Predicted 'G' -> CORRECT\n",
      "MC 1: Predicted 'A' -> INCORRECT\n",
      "MC 2: Predicted 'B' -> INCORRECT\n",
      "MC 3: Predicted 'G' -> CORRECT\n",
      "------------------------------------------------------------\n",
      "\n",
      "Step 0 MC Score: 0.500 (2/4 correct)\n",
      "This means 50.0% of MC continuations from this step reached the correct answer\n"
     ]
    }
   ],
   "source": [
    "# Parse and score each MC response\n",
    "mc_correctness = []\n",
    "mc_details = []\n",
    "\n",
    "ground_truth = str(sample['correct_answer'])\n",
    "print(f\"Ground Truth Answer: {ground_truth}\")\n",
    "print(\"\\nProcessing MC responses:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for mc_idx, mc_response in enumerate(mc_responses):\n",
    "    try:\n",
    "        # Parse the answer from the MC response\n",
    "        # The response should contain the prefix + continuation\n",
    "        full_response = formatted_prefix + mc_response\n",
    "        \n",
    "        # Extract answer using parse_answer function\n",
    "        parsed_answer = parse_answer(full_response, prompt_version='raven_v2')\n",
    "        answer_pred = parsed_answer[-1] if parsed_answer else \"\"\n",
    "        \n",
    "        # Check correctness\n",
    "        correctness = check_answer(\n",
    "            answer_pred=answer_pred,\n",
    "            answer_gt=ground_truth,\n",
    "            mode='raven_score_alphabet_only'\n",
    "        )\n",
    "        \n",
    "        mc_correctness.append(correctness)\n",
    "        mc_details.append({\n",
    "            'mc_idx': mc_idx,\n",
    "            'predicted': answer_pred,\n",
    "            'correct': correctness == 1\n",
    "        })\n",
    "        \n",
    "        print(f\"MC {mc_idx}: Predicted '{answer_pred}' -> {'CORRECT' if correctness == 1 else 'INCORRECT'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MC {mc_idx}: Failed to parse - {e}\")\n",
    "        mc_correctness.append(0)\n",
    "        mc_details.append({\n",
    "            'mc_idx': mc_idx,\n",
    "            'predicted': 'PARSE_ERROR',\n",
    "            'correct': False\n",
    "        })\n",
    "\n",
    "# Calculate step score\n",
    "step_score = sum(mc_correctness) / len(mc_correctness) if mc_correctness else 0.0\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nStep {step_idx} MC Score: {step_score:.3f} ({sum(mc_correctness)}/{len(mc_correctness)} correct)\")\n",
    "print(f\"This means {step_score * 100:.1f}% of MC continuations from this step reached the correct answer\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Complete MC Process for Multiple Steps\n",
    "\n",
    "Let's run the complete MC process for the first few steps to see how scores change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MC evaluation for first 3 steps\n",
      "================================================================================\n",
      "\n",
      "\n",
      "STEP 0 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 1\n",
      "Step Content: I observe the first row of the matrix. Each cell depicts a large circle with a smaller shape inside....\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n",
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MC 0: G -> ✓\n",
      "  MC 1: G -> ✓\n",
      "  MC 2: A -> ✗\n",
      "  MC 3: A -> ✗\n",
      "  MC 4: B -> ✗\n",
      "  MC 5: B -> ✗\n",
      "  MC 6: F -> ✗\n",
      "  MC 7: G -> ✓\n",
      "  MC 8: F -> ✗\n",
      "  MC 9: G -> ✓\n",
      "  MC 10: G -> ✓\n",
      "  MC 11: G -> ✓\n",
      "  MC 12: F -> ✗\n",
      "  MC 13: B -> ✗\n",
      "  MC 14: G -> ✓\n",
      "  MC 15: H -> ✗\n",
      "\n",
      "Step Score: 0.438 (7/16)\n",
      "\n",
      "\n",
      "STEP 1 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 2\n",
      "Step Content: I observe the second row. Each cell now shows a large triangle, containing a smaller shape inside. F...\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n",
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MC 0: A -> ✗\n",
      "  MC 1: C -> ✗\n",
      "  MC 2: C -> ✗\n",
      "  MC 3: G -> ✓\n",
      "  MC 4: F -> ✗\n",
      "  MC 5: E -> ✗\n",
      "  MC 6: G -> ✓\n",
      "  MC 7: A -> ✗\n",
      "  MC 8: A -> ✗\n",
      "  MC 9: B -> ✗\n",
      "  MC 10: B -> ✗\n",
      "  MC 11: A -> ✗\n",
      "  MC 12: G -> ✓\n",
      "  MC 13: B -> ✗\n",
      "  MC 14: G -> ✓\n",
      "  MC 15: C -> ✗\n",
      "\n",
      "Step Score: 0.250 (4/16)\n",
      "\n",
      "\n",
      "STEP 2 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 3\n",
      "Step Content: I observe the third row. Each cell presents a large diamond (square rotated 45 degrees with respect ...\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MC 0: C -> ✗\n",
      "  MC 1: F -> ✗\n",
      "  MC 2: G -> ✓\n",
      "  MC 3: G -> ✓\n",
      "  MC 4: G -> ✓\n",
      "  MC 5: F -> ✗\n",
      "  MC 6: B -> ✗\n",
      "  MC 7: H -> ✗\n",
      "  MC 8: B -> ✗\n",
      "  MC 9: B -> ✗\n",
      "  MC 10: A -> ✗\n",
      "  MC 11: A -> ✗\n",
      "  MC 12: B -> ✗\n",
      "  MC 13: F -> ✗\n",
      "  MC 14: C -> ✗\n",
      "  MC 15: F -> ✗\n",
      "\n",
      "Step Score: 0.188 (3/16)\n",
      "\n",
      "================================================================================\n",
      "MC EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Step 0 (Perception 1): Score = 0.438 (7/16 correct)\n",
      "Step 1 (Perception 2): Score = 0.250 (4/16 correct)\n",
      "Step 2 (Perception 3): Score = 0.188 (3/16 correct)\n",
      "\n",
      "Early Stopping Analysis:\n",
      "No early stopping triggered (all steps have score > 0)\n"
     ]
    }
   ],
   "source": [
    "# Run MC for multiple steps\n",
    "max_steps_to_test = min(3, len(all_steps))  # Test first 3 steps\n",
    "num_mc_per_step = 16\n",
    "steps_with_scores = []\n",
    "\n",
    "print(f\"Running MC evaluation for first {max_steps_to_test} steps\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for step_idx in range(max_steps_to_test):\n",
    "    print(f\"\\n\\nSTEP {step_idx} EVALUATION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Determine if this is a perception or reasoning step\n",
    "    if step_idx < perception_count:\n",
    "        step_type = \"Perception\"\n",
    "        step_num = step_idx + 1\n",
    "    else:\n",
    "        step_type = \"Reasoning\"\n",
    "        step_num = step_idx - perception_count + 1\n",
    "    \n",
    "    print(f\"Step Type: {step_type} Step {step_num}\")\n",
    "    print(f\"Step Content: {all_steps[step_idx][:100]}...\")\n",
    "    \n",
    "    # Build prefix for this step\n",
    "    prefix_steps = all_steps[:step_idx + 1]\n",
    "    \n",
    "    # Format prefix based on step type\n",
    "    if step_idx < perception_count:\n",
    "        # Still in perception phase\n",
    "        formatted_prefix = \"[Perception]\\n\"\n",
    "        for i, step in enumerate(prefix_steps):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "    else:\n",
    "        # In reasoning phase\n",
    "        formatted_prefix = \"[Perception]\\n\"\n",
    "        for i, step in enumerate(parsed['perception_steps']):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "        formatted_prefix += \"\\n[Reasoning]\\n\"\n",
    "        reasoning_steps = prefix_steps[perception_count:]\n",
    "        for i, step in enumerate(reasoning_steps):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "    \n",
    "    # Generate MC continuations\n",
    "    mc_inputs = [(sample['rollout_user_prompt'], sample['image_path'])] * num_mc_per_step\n",
    "    mc_prefixes = [formatted_prefix] * num_mc_per_step\n",
    "    \n",
    "    print(f\"\\nGenerating {num_mc_per_step} MC continuations...\")\n",
    "    \n",
    "    mc_responses = build_responses_azure_simple(\n",
    "        inputs=mc_inputs,\n",
    "        num_return_sequences=1,\n",
    "        prefixes=mc_prefixes,\n",
    "        max_new_tokens=8192,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    \n",
    "    # Score MC responses\n",
    "    mc_correctness = []\n",
    "    for mc_idx, mc_response in enumerate(mc_responses):\n",
    "        try:\n",
    "            full_response = formatted_prefix + mc_response\n",
    "            parsed_answer = parse_answer(full_response, prompt_version='raven_v2')\n",
    "            answer_pred = parsed_answer[-1] if parsed_answer else \"\"\n",
    "            \n",
    "            correctness = check_answer(\n",
    "                answer_pred=answer_pred,\n",
    "                answer_gt=str(sample['correct_answer']),\n",
    "                mode='raven_score_alphabet_only'\n",
    "            )\n",
    "            \n",
    "            mc_correctness.append(correctness)\n",
    "            print(f\"  MC {mc_idx}: {answer_pred} -> {'✓' if correctness == 1 else '✗'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            mc_correctness.append(0)\n",
    "            print(f\"  MC {mc_idx}: Parse error\")\n",
    "    \n",
    "    # Calculate and store step score\n",
    "    step_score = sum(mc_correctness) / len(mc_correctness) if mc_correctness else 0.0\n",
    "    \n",
    "    steps_with_scores.append({\n",
    "        'step_idx': step_idx,\n",
    "        'step_type': step_type,\n",
    "        'step_num': step_num,\n",
    "        'step_content': all_steps[step_idx][:100] + \"...\",\n",
    "        'score': step_score,\n",
    "        'num_correct': sum(mc_correctness),\n",
    "        'num_total': len(mc_correctness)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nStep Score: {step_score:.3f} ({sum(mc_correctness)}/{len(mc_correctness)})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MC EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for step_result in steps_with_scores:\n",
    "    print(f\"Step {step_result['step_idx']} ({step_result['step_type']} {step_result['step_num']}): \"\n",
    "          f\"Score = {step_result['score']:.3f} \"\n",
    "          f\"({step_result['num_correct']}/{step_result['num_total']} correct)\")\n",
    "\n",
    "# Check if early stopping would trigger\n",
    "print(\"\\nEarly Stopping Analysis:\")\n",
    "for i, step_result in enumerate(steps_with_scores):\n",
    "    if step_result['score'] == 0.0:\n",
    "        print(f\"Early stopping would trigger at step {i} (score = 0.0)\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No early stopping triggered (all steps have score > 0)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Detailed Look at Answer Parsing\n",
    "\n",
    "Let's examine how the `parse_answer` and `check_answer` functions work in detail:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER PARSING DEMONSTRATION\n",
      "================================================================================\n",
      "Raw answer section:\n",
      "$\\boxed{A}$\n",
      "\n",
      "------------------------------------------------------------\n",
      "Parsing with parse_answer function:\n",
      "Parsed answers: (None, 'A')\n",
      "Final answer extracted: 'A'\n",
      "\n",
      "------------------------------------------------------------\n",
      "ANSWER CHECKING:\n",
      "Ground truth: 'G'\n",
      "Predicted: 'A'\n",
      "Correctness score: 0\n",
      "Answer is: INCORRECT\n",
      "\n",
      "------------------------------------------------------------\n",
      "SCORING MODE: 'raven_score_alphabet_only'\n",
      "This mode extracts alphabetic characters from the answer and compares them.\n",
      "For example:\n",
      "  - '$\\boxed{D}$' → 'D'\n",
      "  - 'Answer: D' → 'D'\n",
      "  - 'D is correct' → 'D'\n",
      "\n",
      "================================================================================\n",
      "COMPLETE MC ROLLOUT PROCESS SUMMARY:\n",
      "================================================================================\n",
      "1. Generate initial rollout with perception and reasoning steps\n",
      "2. For each step, create a prefix containing all steps up to that point\n",
      "3. Generate multiple MC continuations from each prefix\n",
      "4. Parse the final answer from each MC continuation\n",
      "5. Check correctness against ground truth\n",
      "6. Calculate MC score as (# correct) / (# total MC samples)\n",
      "7. Use scores to evaluate quality of each reasoning step\n",
      "\n",
      "The MC score indicates how likely the model is to reach the correct answer\n",
      "when continuing from that specific step in the reasoning process.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate answer parsing in detail\n",
    "test_response = response_list[0]  # Use the first generated response\n",
    "\n",
    "print(\"ANSWER PARSING DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract the answer section from the response\n",
    "import re\n",
    "answer_pattern = r'<correct_answer>(.*?)</correct_answer>'\n",
    "answer_match = re.search(answer_pattern, test_response, re.DOTALL)\n",
    "\n",
    "if answer_match:\n",
    "    answer_section = answer_match.group(1).strip()\n",
    "    print(f\"Raw answer section:\\n{answer_section}\")\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    \n",
    "    # Parse answer using the parse_answer function\n",
    "    print(\"Parsing with parse_answer function:\")\n",
    "    parsed_answers = parse_answer(test_response, prompt_version='raven_v2')\n",
    "    print(f\"Parsed answers: {parsed_answers}\")\n",
    "    \n",
    "    if parsed_answers:\n",
    "        final_answer = parsed_answers[-1]\n",
    "        print(f\"Final answer extracted: '{final_answer}'\")\n",
    "        \n",
    "        # Check answer correctness\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"ANSWER CHECKING:\")\n",
    "        print(f\"Ground truth: '{sample['correct_answer']}'\")\n",
    "        print(f\"Predicted: '{final_answer}'\")\n",
    "        \n",
    "        correctness = check_answer(\n",
    "            answer_pred=final_answer,\n",
    "            answer_gt=str(sample['correct_answer']),\n",
    "            mode='raven_score_alphabet_only'\n",
    "        )\n",
    "        \n",
    "        print(f\"Correctness score: {correctness}\")\n",
    "        print(f\"Answer is: {'CORRECT' if correctness == 1 else 'INCORRECT'}\")\n",
    "        \n",
    "        # Show what the scoring mode does\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"SCORING MODE: 'raven_score_alphabet_only'\")\n",
    "        print(\"This mode extracts alphabetic characters from the answer and compares them.\")\n",
    "        print(\"For example:\")\n",
    "        print(\"  - '$\\\\boxed{D}$' → 'D'\")\n",
    "        print(\"  - 'Answer: D' → 'D'\")\n",
    "        print(\"  - 'D is correct' → 'D'\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find answer section in response\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE MC ROLLOUT PROCESS SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Generate initial rollout with perception and reasoning steps\")\n",
    "print(\"2. For each step, create a prefix containing all steps up to that point\")\n",
    "print(\"3. Generate multiple MC continuations from each prefix\")\n",
    "print(\"4. Parse the final answer from each MC continuation\")\n",
    "print(\"5. Check correctness against ground truth\")\n",
    "print(\"6. Calculate MC score as (# correct) / (# total MC samples)\")\n",
    "print(\"7. Use scores to evaluate quality of each reasoning step\")\n",
    "print(\"\\nThe MC score indicates how likely the model is to reach the correct answer\")\n",
    "print(\"when continuing from that specific step in the reasoning process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
