{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# RAVEN Monte Carlo Rollout Process Inspection\n",
    "\n",
    "This notebook manually walks through the full Monte Carlo rollout process to help understand how the rollout generation works.\n",
    "\n",
    "## Process Overview:\n",
    "1. Load a sample from RAVEN dataset\n",
    "2. Generate initial rollout responses using the prompt template\n",
    "3. Parse responses into perception and reasoning steps\n",
    "4. Run Monte Carlo rollouts for each step with appropriate prefixes\n",
    "5. Score each MC rollout using parse_answer and check_answer functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Using deployment: gpt-4.1\n",
      "Using endpoint: https://declaregpt4.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "# Import required modules and functions from rollout.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import threading\n",
    "from mimetypes import guess_type\n",
    "from pprint import pprint\n",
    "\n",
    "# Azure OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    "    wait_random\n",
    ")\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Add the parent directory to path to import rollout module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import specific functions from rollout.py (not the API functions)\n",
    "from rollout import (\n",
    "    RAVENDataset,\n",
    "    parse_response_to_perception_and_reasoning_steps_and_correct_answer\n",
    ")\n",
    "\n",
    "# Add the tools directory to import accuracy functions\n",
    "sys.path.append('/data/users/brandon/ob1-projects/InternVL/internvl_chat/tools')\n",
    "from reasoning_data_pipeline.utils.accuracy_reward import check_answer, parse_answer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('rollout_inspection')\n",
    "\n",
    "# Verify Azure API key is set\n",
    "if not os.getenv(\"AZURE_API_KEY\"):\n",
    "    raise ValueError(\"AZURE_API_KEY environment variable not set!\")\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "endpoint = \"https://declaregpt4.openai.azure.com/\"\n",
    "deployment = \"gpt-4.1\"\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "# Create standalone Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_INSPECT_API_KEY\"),\n",
    "    timeout=60.0\n",
    ")\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Using deployment: {deployment}\")\n",
    "print(f\"Using endpoint: {endpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions copied from rollout.py\n",
    "\n",
    "def local_image_to_data_url(image_path):\n",
    "    \"\"\"Convert a local image into data URL\"\"\"\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=2, min=4, max=300) + wait_random(0, 30),\n",
    "    retry=retry_if_exception_type((Exception,)),\n",
    "    reraise=True\n",
    ")\n",
    "def make_azure_request(messages, max_tokens, temperature, estimated_tokens=1000):\n",
    "    \"\"\"Make Azure OpenAI request with retry logic\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            max_completion_tokens=max_tokens,\n",
    "            model=deployment,\n",
    "            temperature=temperature,\n",
    "            timeout=120.0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Check for content filter violation - DO NOT RETRY\n",
    "        if ('BadRequestError' in error_type and \n",
    "            'Error code: 400' in error_msg and \n",
    "            ('ResponsibleAIPolicyViolation' in error_msg or 'content_filter' in error_msg)):\n",
    "            logger.warning(f\"Content filter violation detected: {error_msg}\")\n",
    "            return \"Error code 400: content filter violation returned\"\n",
    "        \n",
    "        logger.debug(f\"API request failed: {error_type}: {error_msg}\")\n",
    "        raise\n",
    "\n",
    "def build_responses_azure_simple(inputs, num_return_sequences=1, prefixes=None, max_new_tokens=4096, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Simplified version of build_responses_azure for inspection purposes\n",
    "    \"\"\"\n",
    "    total_requests = len(inputs) * num_return_sequences\n",
    "    logger.info(f\"Generating {total_requests} responses\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for seq_idx in range(num_return_sequences):\n",
    "        for input_idx, (prompt, image_path) in enumerate(inputs):\n",
    "            try:\n",
    "                # Convert image path to data URL\n",
    "                data_url = local_image_to_data_url(image_path)\n",
    "                \n",
    "                # Prepare messages\n",
    "                content = [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "                ]\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": content}\n",
    "                ]\n",
    "                \n",
    "                # Add prefix if provided - handle both single prefix and list of prefixes\n",
    "                current_prefix = None\n",
    "                if prefixes:\n",
    "                    if isinstance(prefixes, list):\n",
    "                        # Multiple prefixes - use index based on current request\n",
    "                        request_idx = seq_idx * len(inputs) + input_idx\n",
    "                        if request_idx < len(prefixes) and prefixes[request_idx]:\n",
    "                            current_prefix = prefixes[request_idx]\n",
    "                    else:\n",
    "                        # Single prefix for all requests\n",
    "                        current_prefix = prefixes\n",
    "                \n",
    "                if current_prefix:\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": current_prefix})\n",
    "                \n",
    "                # Make the request\n",
    "                response_text = make_azure_request(messages, max_new_tokens, temperature)\n",
    "                responses.append(response_text)\n",
    "                \n",
    "                logger.info(f\"Generated response {len(responses)}/{total_requests}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate response {len(responses)+1}: {e}\")\n",
    "                responses.append(\"\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Load a Sample from RAVEN Dataset\n",
    "\n",
    "Let's load a single sample from the RAVEN dataset to inspect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2000 lines to 1 samples in range [3, 3]\n",
      "Loaded 1 samples from dataset\n",
      "\n",
      "Sample structure:\n",
      "- ID: 9514\n",
      "- Subset/Split: in_center_single_out_center_single\n",
      "- Correct Answer: H\n",
      "- Image Path: /data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/RAVEN/processed_raven_images/in_center_single_out_center_single/9514.jpg\n",
      "\n",
      "Rollout User Prompt Preview:\n",
      "You are an abstract reasoning puzzle expert. The puzzle you will receive is presented in a standard Raven's Progressive Matrices format: a 3×3 matrix of related images, with the bottom-right cell (the ninth tile) missing. There are eight possible answer choices provided separately, and your task is to decide which of those eight images correctly completes the 3×3 matrix pattern.\n",
      "\n",
      "I will provide you with an image containing:\n",
      "- Problem Matrix: An accompanying image that shows the eight tiles and h...\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset path - adjust this to your actual dataset\n",
    "dataset_path = '/data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/RAVEN/raven_processed_jsonl/last_four_jsonl/in_center_single_out_center_single_test.jsonl'\n",
    "\n",
    "# Load the dataset (just first few samples for inspection)\n",
    "dataset = RAVENDataset(\n",
    "    data=dataset_path,\n",
    "    sample_start_idx=3,\n",
    "    sample_end_idx=3  # Just load 3 samples for inspection\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples from dataset\")\n",
    "\n",
    "# Get the first sample\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"- ID: {sample['id']}\")\n",
    "print(f\"- Subset/Split: {sample['subset_split']}\")\n",
    "print(f\"- Correct Answer: {sample['correct_answer']}\")\n",
    "print(f\"- Image Path: {sample['image_path']}\")\n",
    "print(f\"\\nRollout User Prompt Preview:\")\n",
    "print(sample['rollout_user_prompt'][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Generate Initial Rollout Responses\n",
    "\n",
    "Now let's generate initial rollout responses using the Azure OpenAI API. We'll generate 2 responses to see variation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 4 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 4 initial rollout responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/4\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 4 responses\n",
      "\n",
      "================================================================================\n",
      "ROLLOUT RESPONSE 1:\n",
      "================================================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "The matrix is a 3x3 grid. Each cell contains a pair of nested shapes, with the inner shape shaded. The rows and columns exhibit systematic changes in the pairings of the shapes.\n",
      "</step_1>\n",
      "<step_2>\n",
      "First column: All three outer shapes are circles; the inner shapes are pentagons. The pentagon grows larger going down the rows.\n",
      "</step_2>\n",
      "<step_3>\n",
      "Second column: All three outer shapes are pentagons; the inner shapes are smaller pentagons. The inner pentagon increases in size going down the rows.\n",
      "</step_3>\n",
      "<step_4>\n",
      "Third column: All three outer shapes are triangles; the inner shapes are also triangles. The inner triangle grows larger down the rows.\n",
      "</step_4>\n",
      "<step_5>\n",
      "Looking at each row: \n",
      "- Top row: circle contains pentagon; pentagon contains dark pentagon; triangle contains dark triangle.\n",
      "- Middle row: circle with smaller pentagon; pentagon with smaller pentagon; triangle with smaller pentagon.\n",
      "- Bottom row: circle with medium pentagon; pentagon with medium pentagon; m...\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs for rollout generation\n",
    "num_return_sequences = 4  # Generate 2 different rollouts for the same input\n",
    "\n",
    "# Create input list (prompt, image_path)\n",
    "inputs = [(sample['rollout_user_prompt'], sample['image_path'])]\n",
    "\n",
    "print(f\"Generating {num_return_sequences} initial rollout responses...\")\n",
    "\n",
    "# Generate responses using the Azure API\n",
    "response_list = build_responses_azure_simple(\n",
    "    inputs=inputs,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    prefixes=None,  # No prefix for initial generation\n",
    "    max_new_tokens=8192,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(response_list)} responses\")\n",
    "\n",
    "# Display first response\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROLLOUT RESPONSE 1:\")\n",
    "print(\"=\"*80)\n",
    "print(response_list[0][:1000] + \"...\" if len(response_list[0]) > 1000 else response_list[0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Parse Response into Steps\n",
    "\n",
    "Let's parse the response to extract perception steps, reasoning steps, and the final answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed response!\n",
      "\n",
      "Number of perception steps: 6\n",
      "Number of reasoning steps: 8\n",
      "LLM Answer: $\\boxed{F}$\n",
      "Ground Truth Answer: H\n",
      "\n",
      "============================================================\n",
      "FIRST PERCEPTION STEP:\n",
      "============================================================\n",
      "The matrix is a 3x3 grid. Each cell contains a pair of nested shapes, with the inner shape shaded. The rows and columns exhibit systematic changes in the pairings of the shapes.\n",
      "\n",
      "============================================================\n",
      "LAST PERCEPTION STEP:\n",
      "============================================================\n",
      "Answer options: Each choice contains an outer triangle and a variety of possible inner shapes (diamonds, triangles) with different fills and orientations. Option H is a circle, matching the first column, so it's unlikely.\n",
      "\n",
      "============================================================\n",
      "FIRST REASONING STEP:\n",
      "============================================================\n",
      "The columns determine the outmost shape: first column is all circles, second is all pentagons, third is all triangles.\n",
      "\n",
      "============================================================\n",
      "LAST REASONING STEP:\n",
      "============================================================\n",
      "Option F matches: a triangle with a large, solid black inner triangle.\n"
     ]
    }
   ],
   "source": [
    "# Parse the first response\n",
    "response_to_parse = response_list[0]\n",
    "# response_to_parse = \"\"\"[Perception]\\n<step_1>\\nI observe the first row of the matrix. Each cell depicts a large circle with a smaller shape inside. The shapes from left to right: a hexagon, a triangle, and a pentagon.\\n</step_1>\\n<step_2>\\nI observe the second row. Each cell now shows a large triangle, containing a smaller shape inside. From left to right: a pentagon, a hexagon, and a triangle.\\n</step_2>\\n<step_3>\\nI observe the third row. Each cell presents a large diamond (square rotated 45 degrees with respect to the lower edge), with a smaller shape inside. The first contains a triangle, the middle one a hexagon, and the last one is missing.\\n</step_3>\\n<step_4>\\nI look at the answer choices A-H. Each shows a large diamond with a shape inside except D, which has a large circle. Shapes inside are various (hexagons, pentagons, squares) and some are filled, others are not.\\n</step_4>\\n\\n[Reasoning]\\n<step_1>\\nIdentify the logic in each row: The outer shape progresses as: circle (row 1), triangle (row 2), diamond (row 3). The inner shape shifts as well. Track the relationships of the inner shapes across rows and columns.\\n</step_1>\\n<step_2>\\nLook down each column:\\n- 1st column: Hexagon inside circle; pentagon inside triangle; triangle inside diamond.\\n- 2nd column: Triangle inside circle; hexagon inside triangle; hexagon inside diamond.\\n- 3rd column: Pentagon inside circle; triangle inside triangle; ? inside diamond.\\n</step_2>\\n<step_3>\\nNotice the pattern: In each column, the inner shape of the top row moves to the middle row, the middle row inner shape moves to the bottom row, and the bottom row inner shape moves to the top row (cyclical shift).\\n- 1st column: Hexagon (circle) → pentagon (triangle) → triangle (diamond)\\n- 2nd column: Triangle (circle) → hexagon (triangle) → hexagon (diamond)\\n- 3rd column: Pentagon (circle) → triangle (triangle) → ? (diamond)\\nSo, following the shift: Pentagon (circle) → triangle (triangle) → ? (diamond). The answer must have a pentagon inside the diamond.\\n</step_4>\\n<step_4>\\nExamine the answer choices for a diamond containing a pentagon. Option F is a diamond with a pentagon inside.\\n</step_4>\\n\\n<correct_answer>\\n$\\\\boxed{F}$\\n</correct_answer>\\n\"\"\"\n",
    "\n",
    "try:\n",
    "    parsed = parse_response_to_perception_and_reasoning_steps_and_correct_answer(\n",
    "        response_to_parse,\n",
    "        max_perception_steps=12,\n",
    "        max_reasoning_steps=12\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully parsed response!\")\n",
    "    print(f\"\\nNumber of perception steps: {len(parsed['perception_steps'])}\")\n",
    "    print(f\"Number of reasoning steps: {len(parsed['reasoning_steps'])}\")\n",
    "    print(f\"LLM Answer: {parsed['llm_answer']}\")\n",
    "    print(f\"Ground Truth Answer: {sample['correct_answer']}\")\n",
    "    \n",
    "    # Display first perception step\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST PERCEPTION STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['perception_steps'][0])\n",
    "   \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAST PERCEPTION STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['perception_steps'][-1])\n",
    "    \n",
    "    # Display first reasoning step\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIRST REASONING STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['reasoning_steps'][0])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAST REASONING STEP:\")\n",
    "    print(\"=\"*60)\n",
    "    print(parsed['reasoning_steps'][-1])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse response: {e}\")\n",
    "    print(\"\\nResponse format may not match expected structure.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Run Monte Carlo Rollouts for Each Step\n",
    "\n",
    "Now we'll demonstrate the Monte Carlo process. For each step, we:\n",
    "1. Create a prefix containing all steps up to that point\n",
    "2. Generate multiple continuations from that prefix\n",
    "3. Parse answers and check correctness\n",
    "4. Calculate the MC score for that step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Monte Carlo for step 0 (Perception step 1)\n",
      "Will generate 16 MC continuations\n",
      "\n",
      "============================================================\n",
      "PREFIX FOR MC GENERATION:\n",
      "============================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "The matrix is a 3x3 grid. Each cell contains a pair of nested shapes, with the inner shape shaded. The rows and columns exhibit systematic changes in the pairings of the shapes.\n",
      "</step_1>\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's run MC for the first perception step (step index 0)\n",
    "step_idx = 0\n",
    "num_mc_sequences = 16  # Number of MC samples per step\n",
    "\n",
    "# Combine all steps for easier access\n",
    "all_steps = parsed['perception_steps'] + parsed['reasoning_steps']\n",
    "perception_count = len(parsed['perception_steps'])\n",
    "\n",
    "print(f\"Running Monte Carlo for step {step_idx} (Perception step {step_idx + 1})\")\n",
    "print(f\"Will generate {num_mc_sequences} MC continuations\")\n",
    "\n",
    "# Build the prefix for this step (all steps up to and including current step)\n",
    "prefix_steps = all_steps[:step_idx + 1]\n",
    "\n",
    "# Format the prefix according to the expected structure\n",
    "formatted_prefix = \"[Perception]\\n\"\n",
    "for i, step in enumerate(prefix_steps):\n",
    "    formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREFIX FOR MC GENERATION:\")\n",
    "print(\"=\"*60)\n",
    "print(formatted_prefix)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16 MC responses\n",
      "\n",
      "================================================================================\n",
      "MC CONTINUATION 1 (after prefix):\n",
      "================================================================================\n",
      "[Perception]\n",
      "<step_1>\n",
      "The matrix is 3x3, with the bottom-right cell missing. Each cell shows two nested shapes: a large outer shape and a smaller, shaded inner shape.\n",
      "</step_1>\n",
      "<step_2>\n",
      "Row 1 (top row): \n",
      "- Cell 1: Circle outside, hexagon inside (shaded).\n",
      "- Cell 2: Pentagon outside, pentagon inside (shaded).\n",
      "- Cell 3: Triangle outside, diamond inside (shaded).\n",
      "</step_2>\n",
      "<step_3>\n",
      "Row 2 (middle row):\n",
      "- Cell 1: Circle outside, pentagon inside (shaded).\n",
      "- Cell 2: Pentagon outside, hexagon inside (shaded).\n",
      "- Cell 3: Triangle outside, pentagon inside (shaded).\n",
      "</step_3>\n",
      "<step_4>\n",
      "Row 3 (bottom row):\n",
      "- Cell 1: Circle outside, diamond inside (shaded).\n",
      "- Cell 2: Pentagon outside, diamond inside (shaded).\n",
      "- Cell 3: (missing)\n",
      "</step_4>\n",
      "<step_5>\n",
      "Answer set (A-H): Each option features a triangle as the outer shape and a different inner shape (diamond - shaded/unshaded, triangle, square, etc.). Option H features a circle as the outer shape, diamond inside.\n",
      "</step_5>\n",
      "\n",
      "[Reasoning]\n",
      "<step_1>\n",
      "Notice that each ROW has a consistent OUTER shape: \n",
      "- Row 1: Circle, Pentagon, Triangle\n",
      "- Row 2: Circle, Pentagon, Triangle\n",
      "- Row 3: Circle, Pentagon, (should be Triangle)\n",
      "Thus, the bottom-right cell must be a Triangle as the outer shape.\n",
      "</step_1>\n",
      "<step_2>\n",
      "Observe the pattern for the INNER shape in each row:\n",
      "- Row 1 (left to right): hexagon, pentagon, diamond\n",
      "- Row 2: pentagon, hexagon, pentagon\n",
      "- Row 3: diamond, diamond, ?\n",
      "In columns, inner shapes seem to follow: \n",
      "- Col 1: hexagon > pentagon > diamond (steps down one \"side\" each time)\n",
      "- Col 2: pentagon > hexagon > diamond\n",
      "- Col 3: diamond > pentagon > ?\n",
      "Each outer shape in a row corresponds to an inner shape from the previous cell in a stepwise \"cycle.\"\n",
      "</step_3>\n",
      "<step_3>\n",
      "Alternatively, inner shapes in the third column go: diamond, pentagon, ? → likely hexagon.\n",
      "But Row 3 already has diamond (left), diamond (middle). The sequence for inner shapes diagonally is hexagon, hexagon, ? Not a match.\n",
      "Look at the shade: Each triangle in the third column has a shaded and upright (not rotated) inner shape.\n",
      "</step_3>\n",
      "<step_4>\n",
      "Based on the nested shapes, in Row 1, the inner shape steps one \"side\" down from circle (hexagon) to pentagon, then triangle gets a diamond. Row 2, circle contains pentagon, pentagon contains hexagon, triangle contains pentagon. Row 3, circle contains diamond, pentagon contains diamond; triangle should contain a diamond as well for consistency (the shape continues). \n",
      "Thus, the answer should be a triangle containing a shaded diamond, matching the other two diamond-containing cells of the row.\n",
      "</step_4>\n",
      "<step_5>\n",
      "Among the answer options, option C and G both have triangle outer shapes and black-shaded diamond inner shapes, but C is more upright like the pattern, while G is rotated. The consistent orientation for the shaded diamond in the other matrix cells is upright—option C.\n",
      "</step_5>\n",
      "\n",
      "<correct_answer>\n",
      "$\\boxed{C}$\n",
      "</correct_answer>\n"
     ]
    }
   ],
   "source": [
    "# Generate MC continuations from this prefix\n",
    "mc_inputs = [(sample['rollout_user_prompt'], sample['image_path'])] * num_mc_sequences\n",
    "mc_prefixes = [formatted_prefix] * num_mc_sequences\n",
    "\n",
    "print(f\"\\nGenerating {num_mc_sequences} MC continuations...\")\n",
    "\n",
    "mc_responses = build_responses_azure_simple(\n",
    "    inputs=mc_inputs,\n",
    "    num_return_sequences=1,  # 1 response per input (we duplicate inputs instead)\n",
    "    prefixes=mc_prefixes,\n",
    "    max_new_tokens=8192,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(mc_responses)} MC responses\")\n",
    "\n",
    "# Display first MC continuation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MC CONTINUATION 1 (after prefix):\")\n",
    "print(\"=\"*80)\n",
    "print(mc_responses[0][:500] + \"...\" if len(mc_responses[0]) < 500 else mc_responses[0])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Parse MC Answers and Calculate Scores\n",
    "\n",
    "Now we'll extract the final answers from each MC continuation and check their correctness:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Answer: H\n",
      "\n",
      "Processing MC responses:\n",
      "------------------------------------------------------------\n",
      "MC 0: Predicted 'C' -> INCORRECT\n",
      "MC 1: Predicted 'G' -> INCORRECT\n",
      "MC 2: Predicted 'G' -> INCORRECT\n",
      "MC 3: Predicted 'F' -> INCORRECT\n",
      "MC 4: Predicted 'C' -> INCORRECT\n",
      "MC 5: Predicted 'C' -> INCORRECT\n",
      "MC 6: Predicted 'E' -> INCORRECT\n",
      "MC 7: Predicted 'E' -> INCORRECT\n",
      "MC 8: Predicted 'G' -> INCORRECT\n",
      "MC 9: Predicted 'G' -> INCORRECT\n",
      "MC 10: Predicted 'C' -> INCORRECT\n",
      "MC 11: Predicted 'G' -> INCORRECT\n",
      "MC 12: Predicted 'E' -> INCORRECT\n",
      "MC 13: Predicted 'E' -> INCORRECT\n",
      "MC 14: Predicted 'C' -> INCORRECT\n",
      "MC 15: Predicted 'G' -> INCORRECT\n",
      "------------------------------------------------------------\n",
      "\n",
      "Step 0 MC Score: 0.000 (0/16 correct)\n",
      "This means 0.0% of MC continuations from this step reached the correct answer\n"
     ]
    }
   ],
   "source": [
    "# Parse and score each MC response\n",
    "mc_correctness = []\n",
    "mc_details = []\n",
    "\n",
    "ground_truth = str(sample['correct_answer'])\n",
    "print(f\"Ground Truth Answer: {ground_truth}\")\n",
    "print(\"\\nProcessing MC responses:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for mc_idx, mc_response in enumerate(mc_responses):\n",
    "    try:\n",
    "        # Parse the answer from the MC response\n",
    "        # The response should contain the prefix + continuation\n",
    "        full_response = formatted_prefix + mc_response\n",
    "        \n",
    "        # Extract answer using parse_answer function\n",
    "        parsed_answer = parse_answer(full_response, prompt_version='raven_v2')\n",
    "        answer_pred = parsed_answer[-1] if parsed_answer else \"\"\n",
    "        \n",
    "        # Check correctness\n",
    "        correctness = check_answer(\n",
    "            answer_pred=answer_pred,\n",
    "            answer_gt=ground_truth,\n",
    "            mode='raven_score_alphabet_only'\n",
    "        )\n",
    "        \n",
    "        mc_correctness.append(correctness)\n",
    "        mc_details.append({\n",
    "            'mc_idx': mc_idx,\n",
    "            'predicted': answer_pred,\n",
    "            'correct': correctness == 1\n",
    "        })\n",
    "        \n",
    "        print(f\"MC {mc_idx}: Predicted '{answer_pred}' -> {'CORRECT' if correctness == 1 else 'INCORRECT'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MC {mc_idx}: Failed to parse - {e}\")\n",
    "        mc_correctness.append(0)\n",
    "        mc_details.append({\n",
    "            'mc_idx': mc_idx,\n",
    "            'predicted': 'PARSE_ERROR',\n",
    "            'correct': False\n",
    "        })\n",
    "\n",
    "# Calculate step score\n",
    "step_score = sum(mc_correctness) / len(mc_correctness) if mc_correctness else 0.0\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nStep {step_idx} MC Score: {step_score:.3f} ({sum(mc_correctness)}/{len(mc_correctness)} correct)\")\n",
    "print(f\"This means {step_score * 100:.1f}% of MC continuations from this step reached the correct answer\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Complete MC Process for Multiple Steps\n",
    "\n",
    "Let's run the complete MC process for the first few steps to see how scores change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MC evaluation for first 3 steps\n",
      "================================================================================\n",
      "\n",
      "\n",
      "STEP 0 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 1\n",
      "Step Content: The matrix is a 3x3 grid. Each cell contains a pair of nested shapes, with the inner shape shaded. T...\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n",
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MC 0: C -> ✗\n",
      "  MC 1: G -> ✗\n",
      "  MC 2: F -> ✗\n",
      "  MC 3: E -> ✗\n",
      "  MC 4: D -> ✗\n",
      "  MC 5: C -> ✗\n",
      "  MC 6: E -> ✗\n",
      "  MC 7: C -> ✗\n",
      "  MC 8: C -> ✗\n",
      "  MC 9: C -> ✗\n",
      "  MC 10: G -> ✗\n",
      "  MC 11: A -> ✗\n",
      "  MC 12: G -> ✗\n",
      "  MC 13: G -> ✗\n",
      "  MC 14: C -> ✗\n",
      "  MC 15: G -> ✗\n",
      "\n",
      "Step Score: 0.000 (0/16)\n",
      "\n",
      "\n",
      "STEP 1 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 2\n",
      "Step Content: First column: All three outer shapes are circles; the inner shapes are pentagons. The pentagon grows...\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 9/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 10/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 11/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 12/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 13/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 14/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 15/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 16/16\n",
      "INFO:rollout_inspection:Generating 16 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MC 0: G -> ✗\n",
      "  MC 1: C -> ✗\n",
      "  MC 2: G -> ✗\n",
      "  MC 3: C -> ✗\n",
      "  MC 4: G -> ✗\n",
      "  MC 5: F -> ✗\n",
      "  MC 6: F -> ✗\n",
      "  MC 7: F -> ✗\n",
      "  MC 8: G -> ✗\n",
      "  MC 9: C -> ✗\n",
      "  MC 10: C -> ✗\n",
      "  MC 11: G -> ✗\n",
      "  MC 12: G -> ✗\n",
      "  MC 13: C -> ✗\n",
      "  MC 14: G -> ✗\n",
      "  MC 15: G -> ✗\n",
      "\n",
      "Step Score: 0.000 (0/16)\n",
      "\n",
      "\n",
      "STEP 2 EVALUATION\n",
      "------------------------------------------------------------\n",
      "Step Type: Perception Step 3\n",
      "Step Content: Second column: All three outer shapes are pentagons; the inner shapes are smaller pentagons. The inn...\n",
      "\n",
      "Generating 16 MC continuations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 1/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 2/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 3/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 4/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 5/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 6/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 7/16\n",
      "INFO:httpx:HTTP Request: POST https://declaregpt4.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:rollout_inspection:Generated response 8/16\n"
     ]
    }
   ],
   "source": [
    "# Run MC for multiple steps\n",
    "max_steps_to_test = min(3, len(all_steps))  # Test first 3 steps\n",
    "num_mc_per_step = 16\n",
    "steps_with_scores = []\n",
    "\n",
    "print(f\"Running MC evaluation for first {max_steps_to_test} steps\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for step_idx in range(max_steps_to_test):\n",
    "    print(f\"\\n\\nSTEP {step_idx} EVALUATION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Determine if this is a perception or reasoning step\n",
    "    if step_idx < perception_count:\n",
    "        step_type = \"Perception\"\n",
    "        step_num = step_idx + 1\n",
    "    else:\n",
    "        step_type = \"Reasoning\"\n",
    "        step_num = step_idx - perception_count + 1\n",
    "    \n",
    "    print(f\"Step Type: {step_type} Step {step_num}\")\n",
    "    print(f\"Step Content: {all_steps[step_idx][:100]}...\")\n",
    "    \n",
    "    # Build prefix for this step\n",
    "    prefix_steps = all_steps[:step_idx + 1]\n",
    "    \n",
    "    # Format prefix based on step type\n",
    "    if step_idx < perception_count:\n",
    "        # Still in perception phase\n",
    "        formatted_prefix = \"[Perception]\\n\"\n",
    "        for i, step in enumerate(prefix_steps):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "    else:\n",
    "        # In reasoning phase\n",
    "        formatted_prefix = \"[Perception]\\n\"\n",
    "        for i, step in enumerate(parsed['perception_steps']):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "        formatted_prefix += \"\\n[Reasoning]\\n\"\n",
    "        reasoning_steps = prefix_steps[perception_count:]\n",
    "        for i, step in enumerate(reasoning_steps):\n",
    "            formatted_prefix += f\"<step_{i+1}>\\n{step}\\n</step_{i+1}>\\n\"\n",
    "    \n",
    "    # Generate MC continuations\n",
    "    mc_inputs = [(sample['rollout_user_prompt'], sample['image_path'])] * num_mc_per_step\n",
    "    mc_prefixes = [formatted_prefix] * num_mc_per_step\n",
    "    \n",
    "    print(f\"\\nGenerating {num_mc_per_step} MC continuations...\")\n",
    "    \n",
    "    mc_responses = build_responses_azure_simple(\n",
    "        inputs=mc_inputs,\n",
    "        num_return_sequences=1,\n",
    "        prefixes=mc_prefixes,\n",
    "        max_new_tokens=8192,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    \n",
    "    # Score MC responses\n",
    "    mc_correctness = []\n",
    "    for mc_idx, mc_response in enumerate(mc_responses):\n",
    "        try:\n",
    "            full_response = formatted_prefix + mc_response\n",
    "            parsed_answer = parse_answer(full_response, prompt_version='raven_v2')\n",
    "            answer_pred = parsed_answer[-1] if parsed_answer else \"\"\n",
    "            \n",
    "            correctness = check_answer(\n",
    "                answer_pred=answer_pred,\n",
    "                answer_gt=str(sample['correct_answer']),\n",
    "                mode='raven_score_alphabet_only'\n",
    "            )\n",
    "            \n",
    "            mc_correctness.append(correctness)\n",
    "            print(f\"  MC {mc_idx}: {answer_pred} -> {'✓' if correctness == 1 else '✗'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            mc_correctness.append(0)\n",
    "            print(f\"  MC {mc_idx}: Parse error\")\n",
    "    \n",
    "    # Calculate and store step score\n",
    "    step_score = sum(mc_correctness) / len(mc_correctness) if mc_correctness else 0.0\n",
    "    \n",
    "    steps_with_scores.append({\n",
    "        'step_idx': step_idx,\n",
    "        'step_type': step_type,\n",
    "        'step_num': step_num,\n",
    "        'step_content': all_steps[step_idx][:100] + \"...\",\n",
    "        'score': step_score,\n",
    "        'num_correct': sum(mc_correctness),\n",
    "        'num_total': len(mc_correctness)\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nStep Score: {step_score:.3f} ({sum(mc_correctness)}/{len(mc_correctness)})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MC EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for step_result in steps_with_scores:\n",
    "    print(f\"Step {step_result['step_idx']} ({step_result['step_type']} {step_result['step_num']}): \"\n",
    "          f\"Score = {step_result['score']:.3f} \"\n",
    "          f\"({step_result['num_correct']}/{step_result['num_total']} correct)\")\n",
    "\n",
    "# Check if early stopping would trigger\n",
    "print(\"\\nEarly Stopping Analysis:\")\n",
    "for i, step_result in enumerate(steps_with_scores):\n",
    "    if step_result['score'] == 0.0:\n",
    "        print(f\"Early stopping would trigger at step {i} (score = 0.0)\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No early stopping triggered (all steps have score > 0)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Detailed Look at Answer Parsing\n",
    "\n",
    "Let's examine how the `parse_answer` and `check_answer` functions work in detail:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER PARSING DEMONSTRATION\n",
      "================================================================================\n",
      "Raw answer section:\n",
      "$\\boxed{B}$\n",
      "\n",
      "------------------------------------------------------------\n",
      "Parsing with parse_answer function:\n",
      "Parsed answers: (None, 'B')\n",
      "Final answer extracted: 'B'\n",
      "\n",
      "------------------------------------------------------------\n",
      "ANSWER CHECKING:\n",
      "Ground truth: 'G'\n",
      "Predicted: 'B'\n",
      "Correctness score: 0\n",
      "Answer is: INCORRECT\n",
      "\n",
      "------------------------------------------------------------\n",
      "SCORING MODE: 'raven_score_alphabet_only'\n",
      "This mode extracts alphabetic characters from the answer and compares them.\n",
      "For example:\n",
      "  - '$\\boxed{D}$' → 'D'\n",
      "  - 'Answer: D' → 'D'\n",
      "  - 'D is correct' → 'D'\n",
      "\n",
      "================================================================================\n",
      "COMPLETE MC ROLLOUT PROCESS SUMMARY:\n",
      "================================================================================\n",
      "1. Generate initial rollout with perception and reasoning steps\n",
      "2. For each step, create a prefix containing all steps up to that point\n",
      "3. Generate multiple MC continuations from each prefix\n",
      "4. Parse the final answer from each MC continuation\n",
      "5. Check correctness against ground truth\n",
      "6. Calculate MC score as (# correct) / (# total MC samples)\n",
      "7. Use scores to evaluate quality of each reasoning step\n",
      "\n",
      "The MC score indicates how likely the model is to reach the correct answer\n",
      "when continuing from that specific step in the reasoning process.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate answer parsing in detail\n",
    "test_response = response_list[0]  # Use the first generated response\n",
    "\n",
    "print(\"ANSWER PARSING DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract the answer section from the response\n",
    "import re\n",
    "answer_pattern = r'<correct_answer>(.*?)</correct_answer>'\n",
    "answer_match = re.search(answer_pattern, test_response, re.DOTALL)\n",
    "\n",
    "if answer_match:\n",
    "    answer_section = answer_match.group(1).strip()\n",
    "    print(f\"Raw answer section:\\n{answer_section}\")\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    \n",
    "    # Parse answer using the parse_answer function\n",
    "    print(\"Parsing with parse_answer function:\")\n",
    "    parsed_answers = parse_answer(test_response, prompt_version='raven_v2')\n",
    "    print(f\"Parsed answers: {parsed_answers}\")\n",
    "    \n",
    "    if parsed_answers:\n",
    "        final_answer = parsed_answers[-1]\n",
    "        print(f\"Final answer extracted: '{final_answer}'\")\n",
    "        \n",
    "        # Check answer correctness\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"ANSWER CHECKING:\")\n",
    "        print(f\"Ground truth: '{sample['correct_answer']}'\")\n",
    "        print(f\"Predicted: '{final_answer}'\")\n",
    "        \n",
    "        correctness = check_answer(\n",
    "            answer_pred=final_answer,\n",
    "            answer_gt=str(sample['correct_answer']),\n",
    "            mode='raven_score_alphabet_only'\n",
    "        )\n",
    "        \n",
    "        print(f\"Correctness score: {correctness}\")\n",
    "        print(f\"Answer is: {'CORRECT' if correctness == 1 else 'INCORRECT'}\")\n",
    "        \n",
    "        # Show what the scoring mode does\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"SCORING MODE: 'raven_score_alphabet_only'\")\n",
    "        print(\"This mode extracts alphabetic characters from the answer and compares them.\")\n",
    "        print(\"For example:\")\n",
    "        print(\"  - '$\\\\boxed{D}$' → 'D'\")\n",
    "        print(\"  - 'Answer: D' → 'D'\")\n",
    "        print(\"  - 'D is correct' → 'D'\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find answer section in response\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE MC ROLLOUT PROCESS SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Generate initial rollout with perception and reasoning steps\")\n",
    "print(\"2. For each step, create a prefix containing all steps up to that point\")\n",
    "print(\"3. Generate multiple MC continuations from each prefix\")\n",
    "print(\"4. Parse the final answer from each MC continuation\")\n",
    "print(\"5. Check correctness against ground truth\")\n",
    "print(\"6. Calculate MC score as (# correct) / (# total MC samples)\")\n",
    "print(\"7. Use scores to evaluate quality of each reasoning step\")\n",
    "print(\"\\nThe MC score indicates how likely the model is to reach the correct answer\")\n",
    "print(\"when continuing from that specific step in the reasoning process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
